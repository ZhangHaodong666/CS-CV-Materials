{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2-final"},"orig_nbformat":2,"kernelspec":{"name":"python36564bitptcondaa6a0f022e4d049e4af0126dc758592ac","display_name":"Python 3.6.5 64-bit ('pt': conda)"},"colab":{"name":"ニューラルネットワーク.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GZ5ExgCUR9ld","colab_type":"text"},"source":["# 0 Preface\n","このノートブックではニューラルネットワークを用いて，以下の3つの基本的な問題に取り組みます．\n","\n","\n","*   CNNを使った画像分類\n","\n","## 注意\n","このノートブックは，PyTorchの公式チュートリアルである [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html#deep-learning-with-pytorch-a-60-minute-blitz) の [Neural Networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) を日本語に翻訳し，必要に応じて必要事項を適宜補足したものです．"]},{"cell_type":"markdown","metadata":{"id":"K9knimKcR9le","colab_type":"text"},"source":["# ニューラルネットワーク\n","\n","\n","\n","\n","\n","ニューラルネットワークはtorch.nnパッケージを使って構築することができます．\n","前回のノートブックでautogradについて解説しましたが，torch.nnはautogradを用いたモデルを定義します．</br>\n","\n","nn.Moduleはレイヤーと出力を返すforward(input)メソッドを含みます．\n","\n","例として，以下の数字画像を分類するネットワークを見てください．\n","\n","\n","<img src=\"https://pytorch.org/tutorials/_images/mnist.png\" border=\"0\"> \n","\n","\n","\n","これは非常にシンプルなフィードフォワードネットワークの例となっています．具体的には，画像を入力とし，いくつかのレイヤーを次々と伝播した後に，最終的にその画像のクラスを表すone-hotベクトルを出力します．\n","\n","典型的なニューラルネットワークの訓練手順は以下の6ステップから成ります．\n","\n","1. 訓練の対象である重み（パラメータ）をもつニューラルネットワークを定義する\n","\n","2. データセットから入力データを繰り返しネットワークに入力する\n","\n","3. 入力をネットワークに伝播させる\n","\n","4. 最終的な出力が正解データとどれくらい異なるかを表す誤差を計算する\n","\n","5. 誤差をネットワークの各重みに逆伝播させる（バックプロパゲーション）\n","\n","6. 5.で得られた勾配をもとにネットワークの各重みを更新する（最も単純な更新式の例としては， `weight = weight - learning_rate * gradient` など）\n"]},{"cell_type":"markdown","metadata":{"id":"nKisyWy1fr_1","colab_type":"text"},"source":["【**注意**】 以降でdrive中にファイルを読み込めるよう以下を実行しておいてください．"]},{"cell_type":"code","metadata":{"id":"ehKJtKliDaOf","colab_type":"code","colab":{}},"source":["from google.colab import drive # driveを接続\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbJC1fifR9lf","colab_type":"text"},"source":["# ネットワークを定義する\n","では実際にニューラルネットワークを定義してみましょう．"]},{"cell_type":"code","metadata":{"id":"m7I5gKh9R9lg","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, 5)   # 入力のチャネル数は1，出力のチャネル数は6，5x5の畳み込み層\n","        self.conv2 = nn.Conv2d(6, 16, 5)  # 入力のチャネル数は6，出力のチャネル数は16，5x5の畳み込み層\n","        \n","        # 線形層 y = Wx + b\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)   \n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))   # 大きさ(2,2)のMaxプーリング層\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)    # プーリングのWindowサイズの縦と横が同じ大きさなら，タプルではなく単一の値で指定できる\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]   # バッチの次元以外のすべての次元\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","\n","net = Net()\n","print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kCHxNOlOR9ll","colab_type":"text"},"source":["ここで注目すべきは，backwardの計算（誤差逆伝播における重みの勾配の計算）は自分で実装する必要はないということです．forwardの計算のみを実装すれば，autogradによってbackwardの計算は裏側で自動的に組み込まれます．forwardの計算には，Tensorに対する演算を（基本的には）なんでも使うことができます．\n","\n","学習対象となるモデルのパラメータは，net.parameters()で取得することができます．"]},{"cell_type":"code","metadata":{"id":"iTFQgOheR9ll","colab_type":"code","colab":{}},"source":["params = list(net.parameters())\n","print(len(params))\n","print(params[0].size())  # conv1の重みのSize"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"auJAaFLYR9lo","colab_type":"text"},"source":["このネットワークに，大きさ32x32のランダム画像を入力してみます．\n","\n","\n","【**注意**】 MNISTデータセットの画像をこのネットワークに入力するためには，画像サイズを32x32にリサイズする必要があります．"]},{"cell_type":"code","metadata":{"id":"wsHjrg9qR9lq","colab_type":"code","colab":{}},"source":["input = torch.randn(1, 1, 32, 32)\n","out = net(input)\n","print(out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSPmt0cIR9lt","colab_type":"text"},"source":["全ての重みの微分を0にリセットした後，ランダムな値でのlossの勾配をバックプロパゲーションします．"]},{"cell_type":"code","metadata":{"id":"16qPyiKwR9lt","colab_type":"code","colab":{}},"source":["net.zero_grad()\n","out.backward(torch.randn(1, 10))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wmi2DaoAR9lw","colab_type":"text"},"source":["【**注意**】\n","- `torch.nn`はバッチのみをサポートします．`torch.nn`パッケージでは一貫して，1つのサンプルではなく，サンプルのミニバッチを入力として扱うというルールがあります．\n","- 例えば，`nn.Conv2d`では，バッチサイズ x チャネル数 x 縦 x 横 の4次元のTensorを入力として受け付けます． \n","- もし1つのサンプルを用いたい場合は，`input.unsqueeze(0)`とすることで，バッチの次元を加えることで対処します．"]},{"cell_type":"markdown","metadata":{"id":"ItKitwzbR9lw","colab_type":"text"},"source":["先に進む前に，このあたりで今までの確認をしておきます．\n","\n","- **`torch.Tensor`**<br/>\n","多次元配列のクラス．`backward()`などのautogradの操作をメソッドとしてもつ．また，各Tensorオブジェクトは，そのTensorに関する勾配として`grad`プロパティを保持します．\n","\n","- **`nn.Module`**<br/>\n","ネットワークのモジュールのクラス．重みパラメータや各層の初期化，forward層とbackward層などをカプセル化しています．\n","\n","- **`nn.Parameters`**<br/>\n","`nn.Module`に自動的に登録される重みパラメータのクラス．その実態はTensor．"]},{"cell_type":"markdown","metadata":{"id":"oNRRfxLHR9lx","colab_type":"text"},"source":["# Loss Function\n","Loss Functionとは，(output, target)のペアを入力として，output（出力）とtarget（正解）がどれだけ異なるかを表す値（損失）を計算する関数のことです．"]},{"cell_type":"code","metadata":{"id":"WOb-dS5UR9ly","colab_type":"code","colab":{}},"source":["output = net(input)\n","target = torch.randn(10) \n","target = target.view(1, -1)  # targetのshapeをoutputのshapeと同じにしておく\n","criterion = nn.MSELoss()\n","\n","loss = criterion(output, target)\n","print(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gtelf_dLR9l1","colab_type":"text"},"source":["ここで，.grad_fn 属性を使用してlossから各層を逆方向に追ってみると，次のような計算グラフが構成されていることがわかります．"]},{"cell_type":"markdown","metadata":{"id":"fz0NDyk8jLkk","colab_type":"text"},"source":["```\n","input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n","      -> view -> linear -> relu -> linear -> relu -> linear\n","      -> MSELoss\n","      -> loss\n","```"]},{"cell_type":"markdown","metadata":{"id":"GbvfbbPCR9l1","colab_type":"text"},"source":["`loss.backward()`を呼ぶと，lossに関してグラフ全体が微分され，グラフ中の`requires_grad=True`となっているTensorはその勾配を`.grad`プロパティに保持します．"]},{"cell_type":"markdown","metadata":{"id":"p053ka1fR9l2","colab_type":"text"},"source":["# バックプロパゲーション\n","`loss.backward()`を呼ぶだけで，`loss`に関するバックプロパゲーションが行われます．今回のバックプロパゲーションを行う前に，以前のバックプロパゲーションで計算された勾配を0にリセットしておく必要があります．\n","\n","以下では，`loss`に関するバックプロパゲーションを行い，`conv1`のバイアスパラメータの勾配の更新を確認しています．"]},{"cell_type":"code","metadata":{"id":"o2AfYx7OR9l2","colab_type":"code","colab":{}},"source":["print('conv1.bias.grad before backward')\n","print(net.conv1.bias.grad)\n","\n","loss.backward()\n","\n","print('conv1.bias.grad after backward')\n","print(net.conv1.bias.grad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zd3P-YySR9l5","colab_type":"text"},"source":["# 重みの更新\n","実用的な更新式として最も単純なものは**確率的勾配降下法（Stochastic Gradient Descent, SGD）**でしょう．SGDは，以下のような更新式で重みを更新する手法です．\n","\n","```\n","weight = weight - learning_rate * gradient\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"NYQjDJ-sR9l5","colab_type":"text"},"source":["これは次のように簡単なPythonコードで実装できます："]},{"cell_type":"code","metadata":{"id":"K72MfFWUR9l6","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","for f in net.parameters():\n","    f.data.sub_(f.grad.data * learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6Bj9Mg_R9l8","colab_type":"text"},"source":["SGDの他にも，Nesterov-SGD, Adam, RMSPropなどの様々な重みの更新手法が存在し，PyTorchではそれらを`torch.optim`パッケージで管理しています．例えば，SGDを使いたい場合は単に\n","```\n","optim.SGD(net.paramters(), lr=0.01)\n","```\n","とすることで`Optimizer`クラスのインスタンスとしてSGDを生成し，これを使って次のように簡単に重みの更新を行うことができます："]},{"cell_type":"code","metadata":{"id":"ByNFzNneR9l9","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","# optimizerを定義する\n","optimizer = optim.SGD(net.parameters(), lr=0.01)\n","\n","# 訓練のループの中で以下の処理を行う\n","optimizer.zero_grad()   # 各層の重みをゼロで初期化する\n","output = net(input)   # 入力からモデルを通して出力を計算\n","loss = criterion(output, target)    # 損失を計算\n","loss.backward()   # バックプロパゲーション\n","optimizer.step()    # 重みの更新をおおなう"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOG00wptR9l_","colab_type":"text"},"source":["### 注意\n","`optimizer.zero_grad()`を実行して手動で勾配をゼロにリセットする必要があることに注意してください．バックプロパゲーションの項で扱ったように，`grad`には`.backward`メソッドが実行されるたびに勾配が積算されるからです．"]},{"cell_type":"markdown","metadata":{"id":"DPjZ1JbySIlp","colab_type":"text"},"source":["# 課題\n","定義した`MyAlexNet()`クラスを参考にAlexNetを再現してください．<br>\n","\n","<img src='https://miro.medium.com/max/2812/1*bD_DMBtKwveuzIkQTwjKQQ.png'>\n","\n","各数値は特徴量の大きさを表しています. <br>\n","例えば， 左の224という数字は入力画像の縦横のサイズ，3はchannel数（RGB）を表しています． 11はconvolutionのカーネルのサイズとなっていて，stride of 4はstrideが4という意味です．<br>\n","【**注意**】\n","- strideが指定されていないところは`stride=1`で実装してください．\n","- 活性化関数には`nn.ReLU`を使用してください．\n","- denseでは`nn.Linear`を利用してください．\n","- 一番右の1000はクラス数なので解きたい課題に合わせて変更できるように__init__の引数`num_classes`として持っておくと便利です．\n","\n","<details>\n","<summary>\n","課題：ヒント1\n","</summary>\n","画像の縦横サイズの変化からpaddingを計算してください．channel数の変化とカーネルのサイズに注目してください．\n","`nn.Conv2d`を5回繰り返した後，`nn.Linear`を3回行う構成になります．\n","</details>\n","\n","<details>\n","<summary>\n","課題：ヒント2\n","</summary>\n","3回目のmaxpoolingのあと，Full connectionするためにはtensorのshapeを(batch数, 256*6*6)のベクトルに変えた後に(batch数, 4096)への線型層にかけます<br>\n","Net()を参考に変えてください\n","\n","</details>"]},{"cell_type":"code","metadata":{"id":"j993bLgyR9mA","colab_type":"code","colab":{}},"source":["class MyAlexNet(nn.Module):\n","  def __init__(self, num_classes):\n","    super(MyAlexNet, self).__init__()\n","    # TODO\n","\n","  def forward(self, x):\n","    # TODO\n","\n","    return x\n","mynet = MyAlexNet()\n","print(mynet)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zj_42yeo2TQI","colab_type":"text"},"source":["余談ですが，自分が実装しているモデルの中間出力のshapeを知りたいときやパラメータの数を知りたいとき，`torchsummary`を使うと便利です．課題を解く際に使ってみてください"]},{"cell_type":"code","metadata":{"id":"G2Hs1YaCvajT","colab_type":"code","colab":{}},"source":["from torchsummary import summary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhCPR9yGvdSs","colab_type":"code","colab":{}},"source":["mynet = MyAlexNet()\n","summary(mynet, (3, 224, 224))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbeYLE_E29_w","colab_type":"text"},"source":["<details>\n","<summary>\n","解答\n","</summary>\n","\n","    class MyAlexNet(nn.Module):\n","      def __init__(self, num_classes=1000):\n","        super(MyAlexNet, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\n","        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n","        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n","        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc3 = nn.Linear(4096, num_classes)\n","\n","      def forward(self, x):\n","        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = F.max_pool2d(F.relu(self.conv5(x)), 2)\n","        x = x.view(-1, self.num_flat_features(x))\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","      def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","\n","</details>"]}]}