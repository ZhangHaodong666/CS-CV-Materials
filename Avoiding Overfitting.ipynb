{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"過学習と交差検証.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yIqaIuXqb2ty","colab_type":"text"},"source":["# **過学習を緩和する**\n","過学習とはモデルが訓練データに過剰に適合してしまい、テストデータ(未経験のデータ)に対する精度が訓練データに対する精度と比較して劣ってしまう現象のことです。\n","\n","過学習はネットワークのパラメタ数の多さ、訓練データの少なさ、訓練データの偏りなど様々な要因によって引き起こされ、完全になくすことはできません。\n","しかし学習の方法や訓練データの前処理などによって過学習を抑えることはできます。\n","本節では以下のような過学習を抑制するためのテクニックについて実験を行います。\n","\n","* Weight decay\n","* Dropout\n","* Data augmentation"]},{"cell_type":"markdown","metadata":{"id":"ck-NTXiMfdhx","colab_type":"text"},"source":["## **0 準備**\n","まずは実験を行うためのネットワークを準備します。\n","題材には以前に使用したCIFAR10の分類ネットワークを用います。\n","変更点としては学習をGPU上で行うこと、ネットワーク構造が多少変化していることが挙げられます。\n","\n","**この先に進む前にColaboratoryのランタイムタイプがGPUになっていることを確認して下さい。**\n","\n","以下のコードを実行して\"cuda:0\"と表示されればOKです。\n","もし\"cpu\"と表示された場合はColaboratoryのランタイムタイプがCPUになっているため、ランタイムタイプを変更してから再度実行して下さい。"]},{"cell_type":"code","metadata":{"id":"x1HT6X2mit37","colab_type":"code","colab":{}},"source":["import torch\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fn9qBFA5iv0s","colab_type":"text"},"source":["以下のコードを実行すればネットワークを訓練する準備が整います。"]},{"cell_type":"code","metadata":{"id":"kxqRfJP-gQcM","colab_type":"code","colab":{}},"source":["import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","% matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# random seed 設定\n","import random\n","torch.manual_seed(1111)\n","np.random.seed(1111)\n","random.seed(1111)\n","\n","batch_size = 128 #一度に学習するデータ量\n","loss_interval = 50 #test_lossを計算する間隔\n","\n","#変換器の作成\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),  # torch.Tensor へ変換\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # 正規化する\n"," \n","#訓練データのダウンロードと変換設定\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","#訓練データのローダ(読み込み器)の作成\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n"," \n","#テストデータのダウンロードと変換設定\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","#テストデータのローダ(読み込み器)の作成\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=2)\n","\n","#ネットワークの定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\n","        self.conv2 = nn.Conv2d(64, 128, 3, padding = 1)\n","        self.conv3 = nn.Conv2d(128, 256, 3, padding = 1)\n","        self.conv4 = nn.Conv2d(256, 16, 3, padding = 1)\n","        self.pool = nn.MaxPool2d(2)\n","        self.fc1 = nn.Linear(16 * 8 * 8, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 16 * 8 * 8)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","#ネットワークを訓練する関数\n","def train(net, criterion, optimizer, n_epoch = 15):\n","    # Batch normalization や本節で実験を行う Dropout など訓練時と推論時(評価時)に挙動が変化する処理が存在します。\n","    # そのため一般的には訓練前にネットワークを訓練モードに切り替える必要があります。\n","    net.train()  # ネットワークを訓練状態へ切り替える\n","    train_loss = []\n","    test_loss = []\n","    for epoch in range(n_epoch):  # 訓練データを複数回(n_epoch 周分)学習する\n","        for i, data in enumerate(trainloader, 0):\n","            # ローダからデータを読み込む; データは [inputs, labels] の形で取得される\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            # 勾配を0に初期化する(逆伝播に備える)\n","            optimizer.zero_grad()\n","\n","            # 順伝播 + 逆伝播 + 最適化\n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # 統計を計算する\n","            if i % loss_interval == (loss_interval - 1):    # loss_interval ミニバッチ毎に計算する\n","                train_loss.append(loss.item())\n","\n","                # テストデータに対する損失を計算する(訓練はしない)\n","                # net.eval()でネットワークを推論(評価)モードに変更してテストデータを扱う場合もあります。\n","                # ただし本節では過学習について比較を行うので、訓練とテストにおける損失の計算を平等に行うために使用しません。\n","                with torch.no_grad():  #勾配計算をしない宣言(逆伝播用の計算グラフを作成しないことでメモリ節約、速度向上する)\n","                    data = iter(testloader).next()  #  テストデータを1ミニバッチ取得する\n","                    inputs, labels = data[0].to(device), data[1].to(device)\n","                    outputs = net(inputs)\n","                    loss = criterion(outputs, labels)\n","                    test_loss.append(loss.item())\n","\n","        print('epoch {}/{} finished'.format(epoch+1,n_epoch))\n","            \n","    print('Finished Training')\n","    return train_loss, test_loss\n","\n","# 損失の変遷を表示する関数\n","def show_loss(train_loss, test_loss):\n","    plt.xlabel(\"iter\")\n","    plt.ylabel(\"loss\")\n","    x = [i*loss_interval for i in range(len(train_loss))]\n","    plt.plot(x, train_loss, label='train_loss')\n","    plt.plot(x, test_loss, label='test_loss')\n","    plt.legend()\n","    plt.show()\n","\n","# ネットワークの予測精度を計算する関数\n","def check_accuracy(net):\n","    # Batch normalization や本節で実験を行う Dropout など訓練時と推論時(評価時)に挙動が変化する処理が存在します。\n","    # そのため一般的には推論前にネットワークを推論(評価)モードに切り替える必要があります。\n","    net.eval()  # ネットワークを評価状態へ切り替える\n","    ret = []\n","    with torch.no_grad():  #勾配計算をしない宣言(逆伝播用の計算グラフを作成しないことでメモリ節約、速度向上する)\n","        for loader, name in [[trainloader, 'train'], [testloader, 'test']]:\n","            correct = 0\n","            total = 0\n","            for data in loader:\n","                images, labels = data[0].to(device), data[1].to(device)\n","                outputs = net(images)\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","            ret.append(100 * correct / total)\n","            print('Accuracy of the network on the {} images: {:.2f} %'.format(name, ret[-1]))\n","\n","    return ret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZaCYm3CXiDJZ","colab_type":"text"},"source":["## **1 過学習の確認**\n","まずは何も過学習に対する対策を行わずに学習を行い、訓練データとテストデータに対する損失と精度の差を確認します。\n","\n","以下のコードを実行すると学習が開始されます(数分かかります)。"]},{"cell_type":"code","metadata":{"id":"ErnpPZrHihlA","colab_type":"code","colab":{}},"source":["net = Net()  #ネットワークの生成\n","net.to(device)  #ネットワークをGPUへ転送\n","\n","#損失関数と optimizer の定義\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001) # 訓練を促進するため適応的optimizerであるAdamを使用します。\n","\n","train_loss, test_loss = train(net, criterion, optimizer)  # ネットワークを訓練する\n","show_loss(train_loss, test_loss) # 損失の変遷を表示する"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QUtbIvzP1dvD","colab_type":"text"},"source":["訓練データに対する損失である\"train_loss\"とテストデータに対する損失である\"test_loss\"が、学習を進めるにつれ乖離していくことが分かります。\n","損失は低いほど良く予測できていることを表すため、このモデルは訓練データに対して過剰に適合している、つまり**過学習が発生**していることになります。\n","\n","以下のコードを実行すると訓練データとテストデータに対する実際の予測精度が計算されます。"]},{"cell_type":"code","metadata":{"id":"YsUcN3A8kMy4","colab_type":"code","colab":{}},"source":["acc = check_accuracy(net) # ネットワークの予測精度を計算する"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7WrLxfKy5CVB","colab_type":"text"},"source":["実際の予測精度からもモデルが訓練データに過剰適合していることが分かります。"]},{"cell_type":"markdown","metadata":{"id":"rarf9xW5FTTw","colab_type":"text"},"source":["## **2 Weight decay**\n","ここでは過学習を抑制するテクニックの一つである Weight decay の実験を行います。\n","\n","Weight decay とはネットワークの各パラメタ(重み)に制限を加えて学習することで過学習を抑制するテクニックです。具体的にはパラメタ(重み)を$w$、通常の損失を$C(w)$とした場合に新たな損失$C'(w)$を\n","\n","$C'(w) = C(w) + \\frac{λ}{2} \\|w\\|^2$\n","\n","と定義し、これを使用して学習を行います。\n","つまりネットワークの各パラメタ(重み)が大きくなり過ぎないように制限を加えることで過学習の抑制を行います。\n","\n","**Pytorch では Optimizer を定義する際に使用を宣言することで容易に Weight decay を行うことができます。**\n","\n","以下のコードを実行すると Weight decay を使用した学習を開始します(数分かかります)。"]},{"cell_type":"code","metadata":{"id":"5GgPEeIalVck","colab_type":"code","colab":{}},"source":["net = Net()  #ネットワークの生成\n","net.to(device)  #ネットワークをGPUへ転送\n","\n","#損失関数と optimizer の定義\n","criterion = nn.CrossEntropyLoss()\n","optimizer_wd = optim.Adam(net.parameters(), lr=0.001, weight_decay=4e-3) # weight_decayを使用するoptimizer\n","\n","train_loss, test_loss = train(net, criterion, optimizer_wd)  # ネットワークを訓練する\n","show_loss(train_loss, test_loss) # 損失の変遷を表示する\n","acc = check_accuracy(net) # ネットワークの予測精度を計算する関数"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MB20pUD_Vl9Q","colab_type":"text"},"source":["Weight decay を使用しない場合と比較して\"train_loss\"と\"test_loss\"の乖離が抑えられていることが分かります。"]},{"cell_type":"markdown","metadata":{"id":"0w-MJcXdT5DR","colab_type":"text"},"source":["## **3 Dropout**\n","ここでは過学習を抑制するテクニックの一つである Dropout の実験を行います。\n","\n","Dropout とは一定の割合のランダムなノードを無視して学習を行うテクニックです。下図のようにいくつかの色の薄いノードを無視して学習を行います。パラメタ(重み)の更新を行うたびにどのノードを無視するかランダムに再設定します。\n","\n","\n","<img src=\"http://get.secret.jp/pt/file/1590986238.png\" width=500>\n","\n","\n","これによりネットワークの自由度を小さくできるため過学習を抑えることができます。また複数ネットワークの合議によって出力を決定する\"アンサンブル\"と呼ばれる手法の近似であるため汎化性能が上がりやすい(過学習を抑えられる)とも言われています。\n","\n","また推論時(評価時)には全てのノードを使用して計算を行います(Dropoutを無効化する)。\n","\n","**Pytorch で Dropout を使用するためにはネットワークの定義時に組み込む必要があります。**\n","Dropout を組み込む場合は以下のようになります。"]},{"cell_type":"code","metadata":{"id":"Y3nd5l3RS1G4","colab_type":"code","colab":{}},"source":["#Dropoutを使用するネットワークの定義\n","class Net_dropout(nn.Module):\n","    def __init__(self):\n","        super(Net_dropout, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 64, 3, padding = 1)\n","        self.conv2 = nn.Conv2d(64, 128, 3, padding = 1)\n","        self.conv3 = nn.Conv2d(128, 256, 3, padding = 1)\n","        self.conv4 = nn.Conv2d(256, 16, 3, padding = 1)\n","        self.pool = nn.MaxPool2d(2)\n","        self.fc1 = nn.Linear(16 * 8 * 8, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","        # dropoutの定義\n","        self.dropout1 = nn.Dropout2d(p=0.2)\n","        self.dropout2 = nn.Dropout2d(p=0.25)\n","        self.dropout3 = nn.Dropout(p=0.3)\n","        self.dropout4 = nn.Dropout(p=0.35)\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = self.dropout1(x)\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = self.pool(x)\n","        x = self.dropout2(x)\n","        x = x.view(-1, 16 * 8 * 8)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout3(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout4(x)\n","        x = self.fc3(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8kJ1gq5dRuT","colab_type":"text"},"source":["以下のコードを実行すると Dropout を使用した学習を開始します(数分かかります)。"]},{"cell_type":"code","metadata":{"id":"DstVWiqtdcwS","colab_type":"code","colab":{}},"source":["net = Net_dropout()  #Dropoutを使用するネットワークの生成\n","net.to(device)  #ネットワークをGPUへ転送\n","\n","#損失関数と optimizer の定義\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","train_loss, test_loss = train(net, criterion, optimizer)  # ネットワークを訓練する\n","show_loss(train_loss, test_loss) # 損失の変遷を表示する\n","acc = check_accuracy(net) # ネットワークの予測精度を計算する関数"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aiaJAeitr-AD","colab_type":"text"},"source":["Dropout を使用しない場合と比較して\"train_loss\"と\"test_loss\"の乖離が抑えられていることが分かります。"]},{"cell_type":"markdown","metadata":{"id":"sS88G-Dqnigx","colab_type":"text"},"source":["## **4 Data augmentation**\n","\n","ここでは過学習を抑制するテクニックの一つである Data augmentation の実験を行います。\n","\n","Data augmentation とはデータを訓練に使用する際に回転、拡大縮小、明度の調整、ガウスノイズなど様々な手段で微修正するテクニックのことです。\n","Data augmentation を行うことで単純に訓練に使用できる画像の数(バリエーション)が増加し、より良い汎化性能を期待できます。\n","\n","<img src=\"http://get.secret.jp/pt/file/1590986191.png\" width=500>\n","\n","\n","また訓練画像数を増やすというよりもむしろネットワークに物体のバリエーションに関する知識(どこが変化しても概念の同一性が保たれるか)を与えていると考えることもできます。\n","例えばサッカーボールは左右上下反転しても同一性が保たれますが、文字\"b,d,q,p\"などは左右上下反転に対して意味が変化してしまいます。Data augmentation を行う際はどのような変換を行うか十分に注意する必要があります。\n","\n","**Pytorch で Data augmentation を行う場合は dataset を作成する際の変換器に組み込むことができます。**\n","また左右反転などの変換は torchvision で容易に実現できます。\n","\n","以下のコードでランダムに\n","- 左右反転\n","- 拡大縮小回転\n","\n","を行う dataset を作成します。"]},{"cell_type":"code","metadata":{"id":"PawC2vim5I2j","colab_type":"code","colab":{}},"source":["#Data augmentationを行う変換器の作成\n","transform_aug = transforms.Compose(\n","    [transforms.RandomHorizontalFlip(p=0.5),  # 左右反転\n","     transforms.RandomAffine(degrees=0.2, scale=(0.8,1.2)),  # 拡大縮小回転\n","     transforms.ToTensor(),  # torch.Tensor へ変換\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 正規化する\n","     ])\n"," \n","#訓練データのダウンロードと変換設定\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform_aug)\n","#訓練データのローダ(読み込み器)の作成\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n"," \n","#テストデータのダウンロードと変換設定\n","#テストデータにはData augmentationを行わない場合も多いです。\n","#ただし本節では過学習を比較することが目的であるので、テストデータでも訓練データと同じ条件で損失を計算するためにData augmentationを行います。\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform_aug)\n","#テストデータのローダ(読み込み器)の作成\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RcRK50Rz-1Jb","colab_type":"text"},"source":["以下のコードで Data augmentation を使用した訓練データで学習を開始します(数分かかります)。"]},{"cell_type":"code","metadata":{"id":"SEyq25Il9x9H","colab_type":"code","colab":{}},"source":["net = Net()  #ネットワークの生成\n","net.to(device)  #ネットワークをGPUへ転送\n","\n","#損失関数と optimizer の定義\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=0.001)\n","\n","train_loss, test_loss = train(net, criterion, optimizer)  # ネットワークを訓練する\n","show_loss(train_loss, test_loss) # 損失の変遷を表示する\n"," \n","#予測精度計算にはData augmentationを使用しない\n","#変換器をData augmentationしないものへ戻す\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=2)\n","\n","acc = check_accuracy(net) # ネットワークの予測精度を計算する"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5OS1BV3ICGvx","colab_type":"text"},"source":["Data augmentation を使用しない場合と比較して\"train_loss\"と\"test_loss\"の乖離が抑えられていることが分かります。"]},{"cell_type":"markdown","metadata":{"id":"kYKrlYhCjWeg","colab_type":"text"},"source":["# **交差検証**\n","\n","交差検証(Cross Validation)とはモデルの**汎化性能の推定**を行う方法の一つです。\n","本節では特に**K-分割交差検証**(K-fold Cross Validation)について扱います。\n","\n","これまでは\n","1. データ全体を訓練データとテストデータに分割する\n","2. 訓練データでモデルを訓練し、テストデータを使用して評価する\n","\n","という手順でモデルの汎化性能を評価してきました。\n","\n","しかしどのようにデータを訓練とテストに分割するのかによって数値上の汎化性能に差が出てしまいます。これはテストデータに偏ったデータが集まってしまった場合により顕著になります。\n","\n","そこでK-分割交差検証では\n","1. データ全体をK個のグループに分割する\n","2. k個目のグループをテストデータ、残りのグループを訓練データとしてモデルを訓練する\n","3. 2をk = 1 ~ KにわたりK回繰り返す\n","4. 各回モデルのテストデータに対する性能を平均して汎化性能とする\n","\n","という手順を取ります。これに計算量は増加しますが、より正確な汎化性能の評価を行うことができます。\n","\n","\n","<img src=\"http://get.secret.jp/pt/file/1590986112.png \" width=500>"]},{"cell_type":"markdown","metadata":{"id":"COGrU1sOjx4Q","colab_type":"text"},"source":["## **0 準備**\n","\n","K-分割交差検証を行うための準備をします。\n","モデルには先ほどに使用したCIFAR10の分類モデルを使用します。\n","\n","訓練データとテストデータを一度結合してからK個に分割を行います。\n","以下のコードを実行することでCIFAR10のデータセットの結合を行います。"]},{"cell_type":"code","metadata":{"id":"S19ZA9t0-2Fr","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import KFold\n","from torch.utils.data import DataLoader, Dataset, Subset\n"," \n","#訓練データのダウンロードと変換設定\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","#テストデータのダウンロードと変換設定\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                        download=True, transform=transform)\n","#dataset の結合\n","#作成する allset には全データが含まれます。\n","allset = torch.utils.data.ConcatDataset([trainset, testset])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFqW3b_fkQ-p","colab_type":"text"},"source":["## **1 K-分割交差検証**\n","\n","実際にK-分割交差検証を動かしてみます。\n","\n","実行に時間がかかってしまうため今回は少なめのK=3での分割を行います。\n","また学習量も少なめに設定してあります。\n","\n","以下のコードを実行すると3-分割交差検証を開始します(数分かかります)。"]},{"cell_type":"code","metadata":{"id":"hTqpQHtikRqB","colab_type":"code","colab":{}},"source":["#分割数(K)\n","n_splits = 3\n","\n","#分割用関数定義\n","#幾つに分割するか、整列された状態で分割するかランダムに分割するかなどを設定できます。\n","#この関数にデータセットを渡すことで分割を行えます。\n","fold = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n","\n","train_acc_ls = []\n","test_acc_ls = []\n","#fold.split でデータセットの分割を行います。\n","#返値はデータそのものでなくデータを指すインデックスとなります。\n","# for と組み合わせることで各回の訓練インデックスとテストインデックスを順番に取得できます。\n","for fold_idx, (train_idx, test_idx) in enumerate(fold.split(allset)):\n","    \n","    print('fold {}/{}'.format(fold_idx+1,n_splits))\n","    net = Net()  #ネットワークの生成\n","    net.to(device)  #ネットワークをGPUへ転送\n","\n","    #損失関数と optimizer の定義\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(net.parameters(), lr=0.001)\n","    \n","    #trainsetとtestsetの作成\n","    #全データを含む allset を訓練インデックスとテストインデックスに応じて分割します。\n","    #Subset を使用して allset からインデックスに対応するデータを抜き出し、データセットを作成します。\n","    trainloader = DataLoader(Subset(allset, train_idx), shuffle=True, batch_size=batch_size)\n","    testloader = DataLoader(Subset(allset, test_idx), shuffle=True, batch_size=batch_size)\n","\n","    train_loss, test_loss = train(net, criterion, optimizer, n_epoch=5)  # ネットワークを訓練する\n","    show_loss(train_loss, test_loss) # 損失の変遷を表示する\n","    train_acc, test_acc = check_accuracy(net) # ネットワークの予測精度を計算する\n","    train_acc_ls.append(train_acc)\n","    test_acc_ls.append(test_acc)\n","\n","#K-分割交差検証による評価を表示する\n","print('\\n\\nAverage train accuracy : {:.2f} %\\nAverage test accuracy : {:.2f} %'.format(np.average(train_acc_ls),np.average(test_acc_ls)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pFaOCbiFkYW_","colab_type":"text"},"source":["データの分割の仕方によって微妙に汎化性能が異なることが分かります(パラメタの初期値などの影響もあります)。\n","\n","K-分割交差検証ではこれら複数回の結果を平均することで信頼性を高めています。"]}]}