{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"自作関数・自作層・自作損失関数.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EzrX6d9WYkyG","colab_type":"text"},"source":["# 自作関数・自作層・自作損失関数\n","様々な深層学習モデルを実装するにあたり、ライブラリに未だ用意されていない演算を利用したい場合や、既存関数を改良して利用したいという状況は多く訪れます。このノートブックでは、そのような場合に自前でカスタム実装を行う方法を概観します。"]},{"cell_type":"code","metadata":{"id":"rcb1IpxOVNxR","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUUs9TI0bMUx","colab_type":"text"},"source":["## 自作関数\n","PyTorchの演算は自動微分機能を備えているため、基本的には通常のPython関数と同様に任意の演算を関数化することができます。\n","\n","ここでは例として[Mish関数](https://arxiv.org/ftp/arxiv/papers/1908/1908.08681.pdf)を実装してみます。Mish関数は近年ReLUやSwishを種々のタスクにおいて凌駕する性能を達成したとして発表された活性化関数です。\n","\n","\n","$Mish(x)=x \\cdot tanh(softplus(x)) = x \\cdot tanh(ln(1+e^x)).$"]},{"cell_type":"code","metadata":{"id":"sgxIb0uwZVki","colab_type":"code","colab":{}},"source":["def mish(x):\n","  return x * (F.softplus(x)).tanh()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g3CNXcGHVVj9","colab_type":"code","colab":{}},"source":["x = torch.randn(5)\n","print(x)\n","y = mish(x)\n","print(y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhSbpx2QVf6S","colab_type":"text"},"source":["## 自作層\n","パラメータを保有した層として関数を実装したい場合、以下のようにして所望の層を作成することができます。\n","\n","\n","1.   初期化時に必要なパラメータを定義しておく。\n","2.   forward関数を実装する。\n","\n","同じく自動微分機能の恩恵により、逆伝播を顕に記述する必要はありません。\n","torch.nn.Parameterでラップしたtorch.Tensorを用いてパラメータを定義することで、誤差逆伝播の勾配グラフに自動でそれらのパラメータが追加されます。したがって、ユーザは特に意識することなく学習可能なパラメータを設計することができます。\n","\n","例として、先ほどのMish関数が自身でその内部のスケールを学習できるように調整してみましょう。\n","\n","\n","$ParametricMish(x)=x \\cdot tanh(softplus(\\alpha x)) = x \\cdot tanh(ln(1+e^{\\alpha x})).$"]},{"cell_type":"code","metadata":{"id":"BHyBVzqHVcYw","colab_type":"code","colab":{}},"source":["class ParametericMish(nn.Module):\n","  def __init__(self, feature_dims):\n","    super().__init__()\n","    self.alpha = nn.Parameter(torch.ones(feature_dims))\n","  \n","  def forward(self, x):\n","    return x * (F.softplus(self.alpha * x)).tanh()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-TrE7pbW8fT","colab_type":"code","colab":{}},"source":["d = 5\n","pmish = ParametericMish(5)\n","x = torch.randn(5)\n","print(x)\n","y = pmish(x)\n","print(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTJaL1UlXEI0","colab_type":"code","colab":{}},"source":["# 逆伝播してみる\n","y.sum().backward()\n","print(pmish.alpha.grad) # 勾配が計算されていることを確認"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_CgcxvJBYEjK","colab_type":"text"},"source":["## 自作損失関数\n","同様の要領で、損失関数を自作することも可能です。\n","損失関数というとかしこまった印象を受けるかもしれませんが、実際は任意の入力を受け取って損失となるスカラー値を返す関数であり、これまでの内容で問題なく実装できます。\n","基本的に損失関数は評価指標ですので、内部にパラメータを保持することはありません。一方、使い勝手の観点から関数よりもクラスとして定義されることが多いため、ここでは例としてJSDivergenceを実装してみます。\n","\n","JSDivergenceは確率分布間のある種の距離指標であり、KLDivergenceの非対称性を解消するために考案されたものです。\n","\n","入力となる確率分布を$P(x)$, $Q(x)$、2つの確率分布間のKLDivergenceを$D_{KL}(P\\|Q)$としたとき、JSDivergenceは以下のように表されます。\n","\n","$M(x)=\\frac{P(x)+Q(x)}{2},$\n","\n","$D_{JS}(P\\|Q) = \\frac{1}{2}\\left(D_{KL}(P\\|M)+D_{KL}(Q\\|M)\\right).$"]},{"cell_type":"code","metadata":{"id":"axcpZt0dX6WY","colab_type":"code","colab":{}},"source":["class JSDivergence(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","  \n","  def forward(self, p, q):\n","    r\"\"\"\n","    バッチごとにJSDIvergenceを計算し、その平均を全体の損失として返す\n","    p, q: (B, K)\n","    \"\"\"\n","    r = (p + q) / 2\n","    return ((self.kl_divergence(p, r) + self.kl_divergence(q, r)) / 2).mean()\n","  \n","  def kl_divergence(self, p, q):\n","    r\"\"\"\n","    KLDivergenceの計算\n","    \"\"\"\n","    return (p * ((p / q.clamp(min=1e-10)).clamp(min=1e-10)).log()).sum(dim=-1) # nan防止\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"epS_b9RqbPyJ","colab_type":"code","colab":{}},"source":["# B=3, outcome=5 の離散確率分布をランダムに2つ生成\n","p = torch.randn(3, 5)\n","p = F.softmax(p, dim=-1)\n","p.requires_grad = True\n","print(\"[p]\")\n","print(p)\n","print(p.data.sum(dim=-1)) # 各バッチにおいて確率の和が1になることを確認\n","print()\n","\n","q = torch.randn(3, 5)\n","q = F.softmax(q, dim=-1)\n","q.requires_grad = True\n","print(\"[q]\")\n","print(q)\n","print(q.data.sum(dim=-1)) # 各バッチにおいて確率の和が1になることを確認"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jfMlcUulbwXw","colab_type":"code","colab":{}},"source":["js_div = JSDivergence()\n","# 損失の計算\n","loss = js_div(p, q)\n","print(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTVGlOB_cE2J","colab_type":"code","colab":{}},"source":["# 誤差逆伝播\n","loss.backward()\n","print(p.grad)\n","print(q.grad)"],"execution_count":0,"outputs":[]}]}